{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "try:\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLOv5 model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Path to the traffic video you want to test\n",
    "video_path = r'traffic_video_test_1.mp4'\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video file opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file: {video_path}\")\n",
    "    exit()\n",
    "\n",
    "# Get video frame width and height\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object to save the output video\n",
    "output_path = r'output_traffic_detection_rl.mp4'\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n",
    "\n",
    "# Reinforcement Learning setup\n",
    "states = ['low_traffic', 'medium_traffic', 'high_traffic']  # Simplified states\n",
    "actions = ['GREEN', 'ORANGE', 'RED']\n",
    "q_table = np.zeros((len(states), len(actions)))  # Q-table\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.95\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Helper function to determine the state based on vehicle count\n",
    "def get_state(vehicle_count):\n",
    "    if vehicle_count < 5:\n",
    "        return 0  # low_traffic\n",
    "    elif 5 <= vehicle_count < 15:\n",
    "        return 1  # medium_traffic\n",
    "    else:\n",
    "        return 2  # high_traffic\n",
    "\n",
    "# Initialize traffic light\n",
    "current_light = \"RED\"\n",
    "current_state = 0\n",
    "light_timer = time.time()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"End of video reached or failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform YOLOv5 inference on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Get the detected objects' bounding boxes and labels\n",
    "    frame_with_detections = results.render()[0].copy()  # Copy to make the image writable\n",
    "\n",
    "    # Count vehicles detected in the frame\n",
    "    vehicle_count = sum(1 for result in results.xyxy[0] if int(result[-1]) == 2)  # '2' is the class ID for car\n",
    "\n",
    "    # Determine current state\n",
    "    new_state = get_state(vehicle_count)\n",
    "\n",
    "    # RL Decision Making\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        # Explore: Choose a random action\n",
    "        action = random.choice(actions)\n",
    "    else:\n",
    "        # Exploit: Choose the best action based on Q-table\n",
    "        action = actions[np.argmax(q_table[new_state])]\n",
    "\n",
    "    # Apply action\n",
    "    if action == \"GREEN\":\n",
    "        if current_light != \"GREEN\":\n",
    "            current_light = \"GREEN\"\n",
    "            light_timer = time.time()\n",
    "    elif action == \"ORANGE\":\n",
    "        if current_light != \"ORANGE\":\n",
    "            current_light = \"ORANGE\"\n",
    "            light_timer = time.time()\n",
    "    elif action == \"RED\":\n",
    "        if current_light != \"RED\":\n",
    "            current_light = \"RED\"\n",
    "            light_timer = time.time()\n",
    "\n",
    "    # Reward Calculation\n",
    "    reward = -1  # Default negative reward\n",
    "    if current_light == \"GREEN\" and vehicle_count > 10:\n",
    "        reward = 10  # Reward for reducing congestion\n",
    "    elif current_light == \"RED\" and vehicle_count < 5:\n",
    "        reward = 5  # Reward for efficient light usage\n",
    "\n",
    "    # Update Q-table\n",
    "    action_index = actions.index(action)\n",
    "    q_table[current_state, action_index] = (1 - learning_rate) * q_table[current_state, action_index] + \\\n",
    "                                           learning_rate * (reward + discount_factor * np.max(q_table[new_state]))\n",
    "\n",
    "    # Update current state\n",
    "    current_state = new_state\n",
    "\n",
    "    # Display traffic light status on frame\n",
    "    cv2.putText(frame_with_detections, f\"Light: {current_light}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                (0, 255, 0) if current_light == \"GREEN\" else (0, 165, 255) if current_light == \"ORANGE\" else (0, 0, 255), 2)\n",
    "    cv2.putText(frame_with_detections, f\"Vehicles: {vehicle_count}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the frame with detections\n",
    "    cv2.imshow('Traffic Video Object Detection', frame_with_detections)\n",
    "\n",
    "    # Save the frame to the output video\n",
    "    out.write(frame_with_detections)\n",
    "\n",
    "    # Press 'q' to exit the real-time detection\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and output file\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
